Title       : A Qualitative Study of Time-Lagged Recurrent Networks
Type        : Award
NSF Org     : ECS 
Latest
Amendment
Date        : August 5,  1999     
File        : a9732785

Award Number: 9732785
Award Instr.: Continuing grant                             
Prgm Manager: Paul Werbos                             
	      ECS  DIV OF ELECTRICAL AND COMMUNICATIONS SYS
	      ENG  DIRECTORATE FOR ENGINEERING             
Start Date  : September 1,  1998  
Expires     : August 31,  2001     (Estimated)
Expected
Total Amt.  : $116285             (Estimated)
Investigator: Derong Liu   (Principal Investigator current)
Sponsor     : Stevens Inst of Technology
	      Castle Point on Hudson
	      Hoboken, NJ  07030    201/216-5000

NSF Program : 1518      CONTROL, NETWORKS, & COMP INTE
Fld Applictn: 0206000   Telecommunications                      
Program Ref : 0000,OTHR,
Abstract    :
              ECS-9732785
Liu
Time-Lagged Recurrent Networks (TLRN)are known to be more
              powerful than feedforward multilayer neural networks and in nonlinear systems
              identification and control. This project will investigate several fundamentally
              important theoretical and practical questions concerning TLRN's in two
              areas:

Structural adaptivity and robustness analysis: For nonlinear
              function
approximation using neural networks, the behavior of
              time-lagged
recurrent networks will be studied in comparison to feedforward
              neural networks. 
It will be shown that, due to the existence of recurrencies
              in the network,
time-lagged recurrent networks will exhibit behavior
              equivalent to
multilayer feedforward neural networks with time-varying
              parameters.  It is
expected that a multilayer feedforward neural network with
              time-varying
paxameters will exhibit adaptive behavior; and the study will
              show that time-lagged recurrent networks with fixed parameters will generally
              exhibit adaptive
behavior.  A thorough study of how time-lagged recurrent
              networks will
perform under parameter perturbations will also be conducted. 
              Results
will be given as bounds for permissible parameter perturbations
              which
guarantee to retain the desired performance of the network.

Training
              based on energy function approach: Stability of recurrent
Neural networks
              which can be translated into convergent behavior and bounded
signals and
              parameters in the network is of fundamental interest.  The
stability
              properties of tiine-lagged recurrent networks will be studied
using the
              Lyapunov's second method.  In applying the Lyapunov second method, one
needs
              to construct a Lyapunov function (or energy function).  Stability
              is
guaranteed by the fart that the Lyapunov function is positive definite
              and
is (monotonically) decreasing over time.  This stability analysis result
              will
shed new lights on the training of neural networks for systems
              identification,
which will lead to training algorithms with guaranteed
              convergence to the global
minimum or to a solution.

