Title       : Complexity of Neural Networks for Applications
Type        : Award
NSF Org     : DMS 
Latest
Amendment
Date        : August 29,  2001    
File        : a9720145

Award Number: 9720145
Award Instr.: Standard Grant                               
Prgm Manager: Michael H. Steuerwalt                   
	      DMS  DIVISION OF MATHEMATICAL SCIENCES       
	      MPS  DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN
Start Date  : August 15,  1997    
Expires     : March 31,  2002      (Estimated)
Expected
Total Amt.  : $75000              (Estimated)
Investigator: Mark A. Kon mkon@bu.edu  (Principal Investigator current)
Sponsor     : Boston University
	      881 Commonwealth Avenue
	      Boston, MA  021182394    617/353-2000

NSF Program : 1266      APPLIED MATHEMATICS
Fld Applictn: 0000099   Other Applications NEC                  
Program Ref : 0000,9263,OTHR,
Abstract    :
              Kon  9720145       The investigator studies neural network architectures and 
              applications of wavelet techniques to investigate neural  networks' complexity.
               Wavelets have been established as a rich  and useful family of expansion
              functions.  The investigator  studies further the recovery of functions (in
              particular  representations of visual images) from their wavelet transforms. 
              Issues of stability and complexity, which have not up to now been  addressed in
              his proof of the Marr conjecture and related  analysis of the Mallat
              conjecture, are studied.  An important  current question regards the complexity
              of such networks (i.e.,  their essential size) for the completion of desired
              tasks.  The  investigator studies two types, so-called functional and logical 
              networks.  Functional networks have received a good deal of  attention, and a
              coherent theory has established that they are  essentially orthogonal (or more
              general) expansion engines.  The  homology between the structure of networks
              and expansion tasks  has allowed establishing the connection of wavelet
              convergence  results with network complexity issues.  The investigator examines
               the class of so-called logical networks as a needed completion of  available
              network architectures for the execution of intelligent  tasks.  In addition he
              works to show that wavelet-based neural  networks achieve lower bounds on
              complexities of neural nets for  given tasks.  These results move toward a
              general complexity  theory for neural nets on the order of current
              computational  complexity theory for serial and parallel computer
              architectures.  Such a complexity theory is expected to be a hybrid of current 
              discrete and continuous computational complexity theories.       The global
              purpose of this project is a mathematical study  of neural network
              architectures that implement some of the  theoretical complexity results that
              the investigator obtains.  Neural networks as models of parallel distributed
              computing are  currently the leading architectures holdi ng a promise of 
              artificially emulating intelligent systems, as has been indicated  in many of
              their current applications (including mortgage  decisions, commercial stock
              market analysis applications,  chemical and thermal homeostasis control
              systems, satellite image  analysis, etc.).  A major unanswered question in the
              development  of such systems is the fundamental issue of how large a network 
              needs to be in order to perform specific intelligent functions.  One type of
              task that current so-called "functional" neural  architectures have difficulty
              in dealing with is artificial  visual recognition and related tasks involved in
              the general area  of robotics.  This difficulty seems to be an inherent part of
              the  functional neural architectures under current study, and the  investigator
              develops architectures involving so-called "logical"  components, which act
              essentially as algorithmic engines.  In  particular such network architectures
              are necessary for  artificial vision tasks, and prototypes of such tasks are 
              simulated computationally with the aid of graduate students  working on the
              project.  Wavelets are currently considered to be  one of the most useful tools
              for representing the types of  input-output functions implemented in neural
              networks.  A more  general question regarding the complexity and size of neural
               networks accomplishing real-world tasks is addressed through  application of
              wavelet techniques to network construction.  In  particular, functional neural
              networks may achieve their optimal  performance using wavelets as activation
              functions.  There is a  larger question here regarding whether wavelet
              techniques are the  best possible for the implementation of functional neural
              network  architectures, which is a conjecture the investigator has made  and
              investigates.  The computational aspects of the project are  aided by
              associated groups at Howard University and Bryn Mawr  College, the Howard group
              involving a number of graduate  students.
